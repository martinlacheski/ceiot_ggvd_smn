import pandas as pd
from pathlib import Path
import logging
import csv
from datetime import date, datetime

import warnings

# Deshabilitar warnings futuros
warnings.simplefilter(action='ignore', category=FutureWarning)


# Configuraci√≥n de logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("uvicorn")

# Definir rutas para capas Bronce y Plata y Diccionario
BASE_DIR = Path(".").resolve()
RAW_DIR = BASE_DIR / "data" / "raw"
BRONCE_DIR = BASE_DIR / "data" / "bronce"
PLATA_DIR = BASE_DIR / "data" / "plata"
PLATA_DIR.mkdir(parents=True, exist_ok=True)
# Crear carpeta para faltantes si no existe
FALTANTES_DIR = BASE_DIR / "data" / "faltantes"
FALTANTES_DIR.mkdir(parents=True, exist_ok=True)
# Crear carpeta para guardar los metadatos
DICCIONARIO_DIR = BASE_DIR / "data" / "diccionario"
DICCIONARIO_DIR.mkdir(parents=True, exist_ok=True)

# Procesamiento de archivos desde Bronce a Plata
def procesar_exploracion_plata():
    ## Carga inicial de datos
    
    # Cargar todos los archivos CSV de la capa Bronce, excluyendo procesados.csv en el ra√≠z
    archivos = [
        archivo for archivo in BRONCE_DIR.rglob("*.csv")
        if archivo.name != "procesados.csv"
    ]

    dfs = []
    for archivo in archivos:
        df = pd.read_csv(archivo)
        df['estacion_archivo'] = archivo.stem  # Agregar nombre del archivo como identificador de estaci√≥n
        dfs.append(df)

    # Concatenar todos los DataFrames en uno solo
    df_estaciones = pd.concat(dfs, ignore_index=True)
    print(df_estaciones)

    
    ## Normalizaci√≥n y combinaci√≥n de fecha y hora
    
    # Convertir FECHA (DDMMAAAA) a string y formatear como DDMMAAAA
    df_estaciones['FECHA'] = df_estaciones['FECHA'].astype(str).str.zfill(8)

    # Convertir a formato datetime (indicando el formato original)
    df_estaciones['FECHA'] = pd.to_datetime(df_estaciones['FECHA'], format='%d%m%Y', errors='coerce')

    # Asegurar que HORA est√° en n√∫mero entero (algunos datasets los tienen como string)
    df_estaciones['HORA'] = df_estaciones['HORA'].astype(int)

    # Crear columna combinada FECHA_HORA como datetime completo
    df_estaciones['FECHA_HORA'] = df_estaciones['FECHA'] + pd.to_timedelta(df_estaciones['HORA'], unit='h')

    # Guardar el archivo con los datos horarios de las estaciones de la provincia
    archivo_horario = PLATA_DIR / "horario_archivo.csv"

    # Guardar el archivo con datos horarios
    df_estaciones.to_csv(archivo_horario, index=False)
    
    
    ## Agrupaci√≥n diaria de variables por estaci√≥n

    # Agrupar por estaci√≥n y fecha para obtener valores resumen diarios
    df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA']).agg({
        'TEMP': ['mean', 'min', 'max'],
        'PNM': ['mean', 'min', 'max'],
        'HUM': ['mean', 'min', 'max'],
        'DD': 'mean',
        'FF': 'mean'
    }).reset_index()

    # Renombrar columnas para facilitar su uso
    df_estaciones_group.columns = ['ESTACION', 'FECHA',
                                'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',
                                'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',
                                'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',
                                'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']

    
    ## Analisis de cobertura temporal por estacion

    # Crear rango completo de fechas
    rango_fechas = pd.date_range(df_estaciones_group['FECHA'].min(), df_estaciones_group['FECHA'].max())

    # Detectar d√≠as faltantes por estaci√≥n y exportar
    for estacion in df_estaciones_group['ESTACION'].unique():
        fechas_est = pd.to_datetime(df_estaciones_group[df_estaciones_group['ESTACION'] == estacion]['FECHA'].unique())
        dias_faltantes = sorted(set(rango_fechas) - set(fechas_est))

        # Crear archivo por estaci√≥n
        nombre_archivo = FALTANTES_DIR / f'dias_faltantes_{estacion.replace(" ", "_").lower()}.txt'
        with open(nombre_archivo, 'w', encoding='utf-8') as f:
            f.write(f"D√≠as faltantes para estaci√≥n: {estacion}\n")
            f.write(f"Total: {len(dias_faltantes)}\n\n")
            for dia in dias_faltantes:
                f.write(f"{dia.strftime('%Y-%m-%d')}\n")

    ## An√°lisis exploratorio ‚Äì valores m√°ximos, m√≠nimos, promedio diario

    # Verificar valores inv√°lidos en DD (mayores a 360)
    valores_dd_invalidos = df_estaciones[df_estaciones['DD'] > 360]['DD'].unique()
    print("Valores inv√°lidos en DD (mayores a 360):", valores_dd_invalidos, "\n")

    # Crear columna de fecha sin hora para agrupar
    df_estaciones['FECHA_DIA'] = df_estaciones['FECHA'].dt.date

    # Crear DD_VALID con NaN en valores > 360 (para excluir del promedio)
    df_estaciones['DD_VALID'] = df_estaciones['DD'].where(df_estaciones['DD'] <= 360, pd.NA)

    # Paso 3: Agrupar por estaci√≥n y d√≠a, y calcular estad√≠sticas
    df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA_DIA']).agg({
        'TEMP': ['mean', 'min', 'max'],
        'PNM': ['mean', 'min', 'max'],
        'HUM': ['mean', 'min', 'max'],
        'DD_VALID': ['mean'],       # Promedio con valores v√°lidos solamente
        'DD': ['min', 'max'],       # Para conservar min y max sin filtrar
        'FF': ['mean', 'min', 'max']
    }).reset_index()

    # Renombrar columnas para facilitar lectura
    df_estaciones_group.columns = [
        'ESTACION', 'FECHA',
        'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',
        'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',
        'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',
        'WIND_DIR_MEAN',  # <- Esta es la media con valores v√°lidos
        'WIND_DIR_MIN', 'WIND_DIR_MAX',
        'WIND_SPEED_MEAN', 'WIND_SPEED_MIN', 'WIND_SPEED_MAX'
    ]

    # Redondear solo las columnas *_MEAN a 1 decimal
    cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']
    df_estaciones_group[cols_mean] = df_estaciones_group[cols_mean].round(1)

    ## Normalizaci√≥n Min-Max: ¬øPor qu√© la aplicamos?

    # Rellenar valores nulos con forward fill
    df_estaciones_group.ffill(inplace=True)

    # Definir columnas *_MEAN para normalizar
    cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']

    # Aplicar normalizaci√≥n Min-Max y mostrar ejemplos
    for col in cols_mean:
        min_val = df_estaciones_group[col].min()
        max_val = df_estaciones_group[col].max()
        df_estaciones_group[col + '_NORM'] = (df_estaciones_group[col] - min_val) / (max_val - min_val)

    # Exportar la Capa Plata INICIAL:
    df_estaciones_group.to_csv(PLATA_DIR / 'dataset_plata_inicial.csv', index=False)

    # --- Funci√≥n para generar metadatos por columna ---
    def generar_metadatos(df):
        metadatos = []
        for col in df.columns:
            serie = df[col]
            tipo = serie.dtype
            no_nulos = serie.notnull().sum()
            nulos = serie.isnull().sum()
            pct_nulos = (nulos / len(serie)) * 100
            unicos = serie.nunique()
            ejemplo = serie.dropna().iloc[0] if no_nulos > 0 else None
            try:
                minimo = serie.min()
                maximo = serie.max()
            except:
                minimo, maximo = None, None

            metadatos.append({
                'Columna': col,
                'Tipo de dato': str(tipo),
                'Valores no nulos': no_nulos,
                'Valores nulos': nulos,
                '% Nulos': round(pct_nulos, 2),
                'Valor m√≠nimo': minimo,
                'Valor m√°ximo': maximo,
                'Valores √∫nicos': unicos,
                'Ejemplo': ejemplo
            })
        return pd.DataFrame(metadatos)

    # --- Diccionario de variables (manual) ---
    diccionario_vars = pd.DataFrame([
        ["ESTACION", "Nombre de la estaci√≥n meteorol√≥gica", "-", "Agrupaci√≥n principal"],
        ["FECHA", "Fecha de la medici√≥n", "AAAA-MM-DD", "Convertido desde DDMMAAAA"],
        ["TEMP_MIN", "Temperatura m√≠nima diaria", "¬∞C", ""],
        ["TEMP_MEAN", "Temperatura promedio diaria", "¬∞C", ""],
        ["TEMP_MAX", "Temperatura m√°xima diaria", "¬∞C", ""],
        ["PNM_MIN", "Presi√≥n m√≠nima diaria", "hPa", ""],
        ["PNM_MEAN", "Presi√≥n promedio diaria", "hPa", ""],
        ["PNM_MAX", "Presi√≥n m√°xima diaria", "hPa", ""],
        ["HUM_MIN", "Humedad relativa m√≠nima diaria", "%", ""],
        ["HUM_MEAN", "Humedad relativa promedio diaria", "%", ""],
        ["HUM_MAX", "Humedad relativa m√°xima diaria", "%", ""],
        ["WIND_DIR_MEAN", "Direcci√≥n promedio del viento", "¬∞", "Se excluyeron valores > 360"],
        ["WIND_SPEED_MEAN", "Velocidad promedio del viento", "km/h", ""],
        ["TEMP_MEAN_NORM", "TEMP_MEAN normalizada Min-Max", "[0, 1]", "Para an√°lisis multivariado"],
        ["PNM_MEAN_NORM", "PNM_MEAN normalizada Min-Max", "[0, 1]", ""],
        ["HUM_MEAN_NORM", "HUM_MEAN normalizada Min-Max", "[0, 1]", ""],
        ["WIND_DIR_MEAN_NORM", "WIND_DIR_MEAN normalizada Min-Max", "[0, 1]", ""],
        ["WIND_SPEED_MEAN_NORM", "WIND_SPEED_MEAN normalizada Min-Max", "[0, 1]", ""]
    ], columns=["Columna", "Descripci√≥n", "Unidad", "Observaciones"])

    # --- Metadatos generales autom√°ticos ---
    estaciones = df_estaciones['NOMBRE'].unique()
    provincia = estaciones[0].split()[-1].capitalize()
    cobertura_geo = f"Estaciones meteorol√≥gicas de la provincia de {provincia}, Argentina"

    fecha_min = df_estaciones['FECHA'].min().strftime('%Y-%m-%d')
    fecha_max = df_estaciones['FECHA'].max().strftime('%Y-%m-%d')
    cobertura_temporal = f"Desde {fecha_min} hasta {fecha_max}"

    metadatos_generales = pd.DataFrame([
        ["Nombre del conjunto de datos", "misiones_plata"],
        ["Fuente original", "Servicio Meteorol√≥gico Nacional (SMN)"],
        ["Cobertura geogr√°fica", cobertura_geo],
        ["Cobertura temporal", cobertura_temporal],
        ["Frecuencia", "Datos horarios, agregados a diario"],
        ["Unidad de observaci√≥n", "Estaci√≥n meteorol√≥gica"],
        ["Formato de archivo", "CSV, Parquet, TXT (delimitado por tabulaciones)"],
        ["Fecha de procesamiento", date.today().isoformat()],
        ["Responsable del procesamiento", "Equipo de an√°lisis de datos - FIUBA"],
        ["Nivel del dataset", "Capa Plata (datos limpios, normalizados y listos para an√°lisis)"]
    ], columns=["Campo", "Valor"])

    # --- Exportar los tres archivos ---
    metadatos_df = generar_metadatos(df_estaciones_group)
    path_metadatos = DICCIONARIO_DIR / 'metadatos_variables.csv'
    path_diccionario = DICCIONARIO_DIR / 'diccionario_variables.csv'
    path_generales = DICCIONARIO_DIR / 'metadatos_generales.csv'

    metadatos_df.to_csv(path_metadatos, index=False)
    diccionario_vars.to_csv(path_diccionario, index=False)
    metadatos_generales.to_csv(path_generales, index=False)
    
    # Imprimir resumen de exportaci√≥n
    logger.info("‚úÖ Archivos exportados en formato Capa Plata:")
    logger.info(f" - CSV:     {PLATA_DIR / 'dataset_plata_inicial.csv'}")
    logger.info(f"\n Filas exportadas: {len(df_estaciones_group)}")
    logger.info(f" Columnas exportadas: {len(df_estaciones_group.columns)}")
    
def procesar_enriquecimiento_plata():
    archivo_plata = PLATA_DIR / "dataset_plata_inicial.csv"
    archivo_horario = PLATA_DIR / "horario_archivo.csv"
    
    # Cargar el dataset diario
    try:
        df_plata = pd.read_csv(archivo_plata, parse_dates=["FECHA"])
        logger.info("‚úÖ Dataset diario cargado correctamente")
    except FileNotFoundError:
        logger.info("‚ö†Ô∏è El archivo diario no fue encontrado")
        return

    # Cargar el dataset horario
    try:
        df_horario = pd.read_csv(archivo_horario, parse_dates=["FECHA_HORA"])
        logger.info("‚úÖ Dataset horario cargado correctamente")
    except FileNotFoundError:
        logger.info("‚ö†Ô∏è El archivo horario no fue encontrado")
        return
        
    # Generar el rango completo de fechas esperadas
    fechas_totales = pd.date_range(start=df_plata['FECHA'].min(), end=df_plata['FECHA'].max(), freq='D')

    # Obtener todas las combinaciones posibles de fecha y estaci√≥n
    estaciones = df_plata['ESTACION'].unique()
    index_completo = pd.MultiIndex.from_product([fechas_totales, estaciones], names=['FECHA', 'ESTACION'])

    # Reindexar para insertar NaNs expl√≠citos en las fechas faltantes
    df_plata = df_plata.set_index(['FECHA', 'ESTACION']).reindex(index_completo).reset_index()

    # Verificar fechas faltantes (para exportar listado)
    faltantes = df_plata[df_plata.isnull().any(axis=1)][['ESTACION', 'FECHA']]

    if not faltantes.empty:
        faltantes.to_csv(PLATA_DIR / "fechas_faltantes.txt", index=False, sep='\t')
        logger.info(f"‚ö†Ô∏è Fechas faltantes exportadas a: {PLATA_DIR / 'fechas_faltantes.txt'}")
    else:
        logger.info("‚úÖ No se encontraron fechas faltantes")

    ## Relleno con forward fill por estaci√≥n
    
    # Ordenar por estaci√≥n y fecha para aplicar forward fill correctamente
    df_plata_ffill = df_plata.sort_values(['ESTACION', 'FECHA']).copy()
    df_plata_ffill.update(df_plata.groupby('ESTACION').ffill())
    
    ## Imputaci√≥n con la media de cada estaci√≥n (solo para columnas num√©ricas)
    
    # Imputar con la media por estaci√≥n
    columnas_a_imputar = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_SPEED_MEAN', 'WIND_DIR_MEAN']

    for col in columnas_a_imputar:
        df_plata_ffill[col] = df_plata_ffill.groupby('ESTACION')[col].transform(lambda x: x.fillna(x.mean()))
    
    ## Tratamiento de datos faltantes en el dataset horario
    
    # Detectar horarios reales de cada estaci√≥n
    df_horario['HORA'] = df_horario['FECHA_HORA'].dt.hour
    horarios_por_estacion = df_horario.groupby('NOMBRE')['HORA'].value_counts().unstack(fill_value=0)
    horarios_mas_frecuentes = horarios_por_estacion.idxmax(axis=1)

    # Detectar horarios outlier (menos del 5% de los d√≠as)
    outliers_horarios = {}
    for estacion in horarios_por_estacion.index:
        total_dias = df_horario[df_horario['NOMBRE'] == estacion]['FECHA_HORA'].dt.date.nunique()
        outliers = horarios_por_estacion.loc[estacion][
            horarios_por_estacion.loc[estacion] / total_dias < 0.05
        ].index.tolist()
        if outliers:
            outliers_horarios[estacion] = outliers

    # Crear index completo por estaci√≥n y sus horarios t√≠picos
    df_horario['FECHA'] = df_horario['FECHA_HORA'].dt.floor('D')
    estaciones_h = df_horario['NOMBRE'].unique()
    fecha_h_min = df_horario['FECHA'].min()
    fecha_h_max = df_horario['FECHA'].max()
    # rango_fechas = pd.date_range(start=fecha_h_min, end=fecha_h_max, freq='D')
    if pd.isna(fecha_h_min) or pd.isna(fecha_h_max):
        logger.warning("‚ö†Ô∏è No se puede generar rango de fechas en horario: FECHA contiene NaT")
        return  # o manejar de otra forma

    rango_fechas = pd.date_range(start=fecha_h_min, end=fecha_h_max, freq='D')

    # Crear combinaciones v√°lidas por estaci√≥n
    porcentaje_frecuencia = 0.05 # al menos en 5% de los d√≠as

    index_completo_personalizado = []
    for estacion in estaciones_h:
        total_dias_estacion = df_horario[df_horario['NOMBRE'] == estacion]['FECHA'].nunique()
        horas_validas = horarios_por_estacion.columns[
            (horarios_por_estacion.loc[estacion] / total_dias_estacion) >= porcentaje_frecuencia  
        ].tolist()

        for fecha in rango_fechas:
            for hora in horas_validas:
                index_completo_personalizado.append((estacion, pd.Timestamp(fecha + pd.Timedelta(hours=hora))))

    index_completo_h = pd.MultiIndex.from_tuples(index_completo_personalizado, names=['NOMBRE', 'FECHA_HORA'])

    # Reindexar para insertar valores faltantes en los horarios esperados √∫nicamente
    df_horario_completo = df_horario.set_index(['NOMBRE', 'FECHA_HORA']).reindex(index_completo_h).reset_index()
    
    # Exportar datasets intermedios (si se desea conservar)
    df_plata.to_csv(PLATA_DIR / "dataset_intermedio_horario_con_nan.csv", index=False)
    df_plata_ffill.to_csv(PLATA_DIR / "dataset_intermedio_horario_ffill.csv", index=False)
    df_horario_completo.to_csv(PLATA_DIR / "dataset_intermedio_horario_completo.csv", index=False)

    logger.info("‚úÖ Dataset intermedios generados correctamente")
    
    ## Imputaci√≥n de datos faltantes basada en promedio entre d√≠as anterior y posterior
    
    # Variables a imputar
    variables_objetivo = ['TEMP', 'HUM', 'PNM', 'DD', 'FF']

    df_interp = df_horario_completo.copy()

    # Asegurar FECHA y HORA correctas
    df_interp['FECHA'] = df_interp['FECHA_HORA'].dt.date
    df_interp['HORA'] = df_interp['FECHA_HORA'].dt.hour

    # Ordenar por estaci√≥n, fecha y hora
    df_interp = df_interp.sort_values(by=['NOMBRE', 'FECHA', 'HORA'])

    # Funci√≥n de imputaci√≥n por promedio entre d√≠a anterior y posterior
    def imputar_valores(grupo):
        grupo = grupo.copy()  # para evitar advertencias de SettingWithCopy
        for var in variables_objetivo:
            for idx, fila in grupo.iterrows():
                if pd.isna(fila[var]):
                    hora = fila['HORA']
                    fecha = fila['FECHA']

                    # Buscar el valor del d√≠a anterior
                    val_ant = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] < fecha)][var].last_valid_index()
                    val_ant = grupo.at[val_ant, var] if val_ant is not None else None

                    # Buscar el valor del d√≠a posterior
                    val_post = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] > fecha)][var].first_valid_index()
                    val_post = grupo.at[val_post, var] if val_post is not None else None

                    # Asignar promedio o valor disponible
                    if val_ant is not None and val_post is not None:
                        grupo.at[idx, var] = round((val_ant + val_post) / 2, 1)
                    elif val_ant is not None:
                        grupo.at[idx, var] = val_ant
                    elif val_post is not None:
                        grupo.at[idx, var] = val_post
        return grupo

    # Aplicar por estaci√≥n SIN include_groups
    df_interp = (
        df_interp.groupby('NOMBRE', group_keys=False)
        .apply(imputar_valores)
        .reset_index(drop=True)
    )

    # Redondear valores num√©ricos a 1 decimal
    for var in variables_objetivo:
        df_interp[var] = df_interp[var].round(1)

    # Ajustar tipos de columnas
    df_interp['HORA'] = df_interp['HORA'].astype('int64')
    if 'estacion_archivo' in df_interp.columns:
        df_interp['estacion_archivo'] = df_interp['estacion_archivo'].astype('int64', errors='ignore')

    # Exportar
    archivo_imputado = PLATA_DIR / "dataset_plata_horario_final.csv"
    df_interp.to_csv(archivo_imputado, index=False)
    logger.info(f"‚úÖ Dataset horario final generado correctamente: {archivo_imputado}")
    
    ## Generar dataset diario imputado (todas las estaciones)

    # Agrupar por estaci√≥n y fecha
    df_diario_imputado = df_interp.groupby(['NOMBRE', 'FECHA']).agg(
        TEMP_MEAN=('TEMP', 'mean'),
        TEMP_MIN=('TEMP', 'min'),
        TEMP_MAX=('TEMP', 'max'),
        PNM_MEAN=('PNM', 'mean'),
        PNM_MIN=('PNM', 'min'),
        PNM_MAX=('PNM', 'max'),
        HUM_MEAN=('HUM', 'mean'),
        HUM_MIN=('HUM', 'min'),
        HUM_MAX=('HUM', 'max'),
        WIND_DIR_MEAN=('DD', 'mean'),
        WIND_DIR_MIN=('DD', 'min'),
        WIND_DIR_MAX=('DD', 'max'),
        WIND_SPEED_MEAN=('FF', 'mean'),
        WIND_SPEED_MIN=('FF', 'min'),
        WIND_SPEED_MAX=('FF', 'max')
    ).reset_index()

    # Renombrar y ordenar
    df_diario_imputado.rename(columns={'NOMBRE':'ESTACION'}, inplace=True)
    df_diario_imputado['FECHA'] = pd.to_datetime(df_diario_imputado['FECHA'])
    df_diario_imputado = df_diario_imputado.sort_values(by=['ESTACION','FECHA']).reset_index(drop=True)

    # Ajustes de tipos y redondeo

    # Redondear medias a 1 decimal
    cols_float = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_DIR_MEAN','WIND_SPEED_MEAN']
    df_diario_imputado[cols_float] = df_diario_imputado[cols_float].round(1)

    # Convertir min y max a enteros
    cols_int = [
        'TEMP_MIN','TEMP_MAX','PNM_MIN','PNM_MAX',
        'HUM_MIN','HUM_MAX',
        'WIND_DIR_MIN','WIND_DIR_MAX',
        'WIND_SPEED_MIN','WIND_SPEED_MAX'
    ]
    df_diario_imputado[cols_int] = df_diario_imputado[cols_int].round().astype(int)

    # Normalizaci√≥n Min-Max de las variables MEAN

    variables_mean = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_DIR_MEAN','WIND_SPEED_MEAN']
    for var in variables_mean:
        col_norm = var + '_NORM'
        min_val = df_diario_imputado[var].min()
        max_val = df_diario_imputado[var].max()
        df_diario_imputado[col_norm] = ((df_diario_imputado[var] - min_val) / (max_val - min_val)).round(5)

    # Guardar el dataset diario imputado completo
    archivo_diario_imputado = PLATA_DIR / "dataset_plata_diario_final.csv"
    df_diario_imputado.to_csv(archivo_diario_imputado, index=False)
    logger.info(f"‚úÖ Dataset diario final generado correctamente: {archivo_diario_imputado}")
    
    # MARCADOR DE PROCESADO EN PLATA
    try:
        marker_csv = PLATA_DIR / "procesados.csv"

        # Rango/filas desde los DataFrames en memoria
        d_fecha_min = pd.to_datetime(df_diario_imputado["FECHA"]).min()
        d_fecha_max = pd.to_datetime(df_diario_imputado["FECHA"]).max()
        d_rows = int(len(df_diario_imputado))

        if "FECHA_HORA" in df_interp.columns:
            h_fh = pd.to_datetime(df_interp["FECHA_HORA"])
        else:
            h_fh = pd.to_datetime(df_interp["FECHA"]) + pd.to_timedelta(df_interp["HORA"], unit="h")
        h_fecha_hora_min = h_fh.min()
        h_fecha_hora_max = h_fh.max()
        h_rows = int(len(df_interp))

        # Mismo timestamp para ambas filas (una por dataset)
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        header = ["timestamp", "dataset", "start", "end", "rows"]
        rows = [
            [ts, "plata_diario_final",
            d_fecha_min.isoformat() if pd.notna(d_fecha_min) else None,
            d_fecha_max.isoformat() if pd.notna(d_fecha_max) else None,
            d_rows],
            [ts, "plata_horario_final",
            h_fecha_hora_min.isoformat() if pd.notna(h_fecha_hora_min) else None,
            h_fecha_hora_max.isoformat() if pd.notna(h_fecha_hora_max) else None,
            h_rows],
        ]

        # Crear carpeta y escribir en modo append (NO at√≥mico)
        marker_csv.parent.mkdir(parents=True, exist_ok=True)
        write_header = not marker_csv.exists() or marker_csv.stat().st_size == 0
        with open(marker_csv, "a", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            if write_header:
                w.writerow(header)
            w.writerows(rows)

        logger.info(
            "üìù Marker de Plata (append) ‚Üí %s | Diario %s‚Üí%s (%s filas), Horario %s‚Üí%s (%s filas)",
            marker_csv,
            d_fecha_min.date() if pd.notna(d_fecha_min) else "NA",
            d_fecha_max.date() if pd.notna(d_fecha_max) else "NA", d_rows,
            h_fecha_hora_min, h_fecha_hora_max, h_rows
        )
    except Exception as e:
        logger.exception("‚ùå No se pudo generar procesados.csv en Plata: %s", e)
    
if __name__ == "__main__":
    procesar_exploracion_plata()
    procesar_enriquecimiento_plata()

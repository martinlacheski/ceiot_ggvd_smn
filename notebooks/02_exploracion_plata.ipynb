{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a61d964",
   "metadata": {},
   "source": [
    "# Exploraci√≥n de datos y preparaci√≥n (Capa Plata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065f283-72b8-44aa-8280-2b0793b03857",
   "metadata": {},
   "source": [
    "En esta clase trabajaremos con los datos meteorol√≥gicos previamente filtrados por estaci√≥n (Capa Bronce) para realizar una primera exploraci√≥n estructurada. El objetivo es transformar los datos crudos en un conjunto limpio, validado y enriquecido, listo para su an√°lisis.\n",
    "\n",
    "## Objetivos de la clase:\n",
    "- Identificar tipos de an√°lisis exploratorio.\n",
    "- Aplicar transformaciones b√°sicas en la Capa Plata.\n",
    "- Detectar y tratar valores nulos y at√≠picos.\n",
    "- Generar estad√≠sticas descriptivas por estaci√≥n y fecha.\n",
    "- Visualizar las principales variables meteorol√≥gicas.\n",
    "- Exportar el dataset en m√∫ltiples formatos para su posterior an√°lisis.\n",
    "\n",
    "El foco estar√° puesto en la limpieza, validaci√≥n, estandarizaci√≥n y normalizaci√≥n de datos para facilitar su consumo en dashboards, an√°lisis y modelos posteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256ad83",
   "metadata": {},
   "source": [
    "## Importaci√≥n de librer√≠as y configuraci√≥n de paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import os\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from windrose import WindroseAxes\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Definir rutas para capas Bronce y Plata y Diccionario\n",
    "BRONCE_DIR = Path('../data/bronce/')\n",
    "PLATA_DIR = Path('../data/plata/')\n",
    "PLATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Crear carpeta para faltantes si no existe\n",
    "FALTANTES_DIR = Path('../data/faltantes/')\n",
    "FALTANTES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Crear carpeta para guardar los metadatos\n",
    "DICCIONARIO_DIR = Path('../data/diccionario/')\n",
    "DICCIONARIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Variable de la provincia para generar los metadatos\n",
    "PROVINCIA = 'Misiones'\n",
    "\n",
    "# Ajustar el ancho m√°ximo para impresi√≥n en consola\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas\n",
    "pd.set_option('display.width', 300)         # Ajustar a un ancho suficiente en consola\n",
    "pd.set_option('display.max_colwidth', None) # Evitar recortes en contenido de celdas\n",
    "\n",
    "print(\"Importaci√≥n de librer√≠as completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863caef1",
   "metadata": {},
   "source": [
    "## Carga de archivos de todas las estaciones de la provincia seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ff7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los archivos CSV de la capa Bronce (filtrados por estaciones)\n",
    "archivos = list(BRONCE_DIR.rglob(\"*.csv\"))\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos:\n",
    "    df = pd.read_csv(archivo)\n",
    "    df['estacion_archivo'] = archivo.stem  # Agregar nombre del archivo como identificador de estaci√≥n\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_estaciones = pd.concat(dfs, ignore_index=True)\n",
    "print(df_estaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624f8c9-d827-4a6c-9913-24c4dbaf3f3b",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n y combinaci√≥n de fecha y hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57b8c9-7b45-40bd-9a35-77eb9dd91109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir FECHA (DDMMAAAA) a string y formatear como DDMMAAAA\n",
    "df_estaciones['FECHA'] = df_estaciones['FECHA'].astype(str).str.zfill(8)\n",
    "\n",
    "# Convertir a formato datetime (indicando el formato original)\n",
    "df_estaciones['FECHA'] = pd.to_datetime(df_estaciones['FECHA'], format='%d%m%Y', errors='coerce')\n",
    "\n",
    "# Asegurar que HORA est√° en n√∫mero entero (algunos datasets los tienen como string)\n",
    "df_estaciones['HORA'] = df_estaciones['HORA'].astype(int)\n",
    "\n",
    "# Crear columna combinada FECHA_HORA como datetime completo\n",
    "df_estaciones['FECHA_HORA'] = df_estaciones['FECHA'] + pd.to_timedelta(df_estaciones['HORA'], unit='h')\n",
    "\n",
    "# Verificamos el resultado\n",
    "print(df_estaciones[['FECHA', 'HORA', 'FECHA_HORA']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898fef1-6cb5-45a4-80e7-81dc88bac3bb",
   "metadata": {},
   "source": [
    "## Guardar el archivo con los datos horarios de las estaciones de la provincia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4f8d6-241a-4051-8bcd-9b54ebc96f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_horario = PLATA_DIR / \"horario_archivo.csv\"\n",
    "\n",
    "# Guardar el archivo con datos horarios\n",
    "df_estaciones.to_csv(archivo_horario, index=False)\n",
    "\n",
    "print(f\"Archivo horario exportado correctamente a:\\n{archivo_horario}\")\n",
    "print(f\"Filas: {len(df_estaciones)} ‚Äì Columnas: {len(df_estaciones.columns)}\")\n",
    "print(\"\\nVista previa de datos horarios:\")\n",
    "print(df_estaciones.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744eff5-6cde-45bc-bb75-4127194aecd0",
   "metadata": {},
   "source": [
    "### Agrupaci√≥n diaria de variables por estaci√≥n\n",
    "\n",
    "En esta secci√≥n agrupamos los datos meteorol√≥gicos por estaci√≥n y por fecha. \n",
    "Calculamos estad√≠sticas clave como temperatura, presi√≥n, humedad, direcci√≥n y velocidad del viento. \n",
    "Esto permite obtener un resumen diario limpio para cada estaci√≥n, facilitando su an√°lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f320826-bf84-4874-98e9-9753fd2064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por estaci√≥n y fecha para obtener valores resumen diarios\n",
    "df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA']).agg({\n",
    "    'TEMP': ['mean', 'min', 'max'],\n",
    "    'PNM': ['mean', 'min', 'max'],\n",
    "    'HUM': ['mean', 'min', 'max'],\n",
    "    'DD': 'mean',\n",
    "    'FF': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas para facilitar su uso\n",
    "df_estaciones_group.columns = ['ESTACION', 'FECHA',\n",
    "                               'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',\n",
    "                               'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',\n",
    "                               'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',\n",
    "                               'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "\n",
    "print(df_estaciones_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05024bce-60cc-4c68-9215-db052e99f05b",
   "metadata": {},
   "source": [
    "## Validaci√≥n estructural: detecci√≥n de duplicados\n",
    "\n",
    "Antes de avanzar con el an√°lisis, verificamos si existen filas duplicadas por combinaci√≥n de estaci√≥n y fecha, lo cual indicar√≠a problemas en la agregaci√≥n de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade6bae-34ef-4c14-94b8-a0be009ad085",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados = df_estaciones_group.duplicated(subset=['ESTACION', 'FECHA']).sum()\n",
    "print(f\" Duplicados por estaci√≥n y fecha: {duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4c05b-242d-45f3-af9b-ef3882dc2b32",
   "metadata": {},
   "source": [
    "## An√°lisis de cobertura temporal por estaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296092-3728-4a57-a340-c4d71a6fd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear rango completo de fechas\n",
    "rango_fechas = pd.date_range(df_estaciones_group['FECHA'].min(), df_estaciones_group['FECHA'].max())\n",
    "\n",
    "# Detectar d√≠as faltantes por estaci√≥n y exportar\n",
    "for estacion in df_estaciones_group['ESTACION'].unique():\n",
    "    fechas_est = pd.to_datetime(df_estaciones_group[df_estaciones_group['ESTACION'] == estacion]['FECHA'].unique())\n",
    "    dias_faltantes = sorted(set(rango_fechas) - set(fechas_est))\n",
    "\n",
    "    # Crear archivo por estaci√≥n\n",
    "    nombre_archivo = FALTANTES_DIR / f'dias_faltantes_{estacion.replace(\" \", \"_\").lower()}.txt'\n",
    "    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"D√≠as faltantes para estaci√≥n: {estacion}\\n\")\n",
    "        f.write(f\"Total: {len(dias_faltantes)}\\n\\n\")\n",
    "        for dia in dias_faltantes:\n",
    "            f.write(f\"{dia.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "    # Mostrar resumen por consola\n",
    "    print(f\" Estaci√≥n: {estacion}\")\n",
    "    print(f\" D√≠as faltantes: {len(dias_faltantes)}\")\n",
    "    print(f\" Archivo: {nombre_archivo.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2293c-32af-4441-9636-f27c7a859216",
   "metadata": {},
   "source": [
    "## Exploraci√≥n univariada de variables meteorol√≥gicas\n",
    "\n",
    "A continuaci√≥n, se presentan histogramas para explorar la distribuci√≥n de variables meteorol√≥gicas clave. Esto permite identificar posibles sesgos, valores extremos o asimetr√≠as en la distribuci√≥n de cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa120573-e103-4a82-a748-53b573c929c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a graficar\n",
    "variables_hist = {\n",
    "    'TEMP_MEAN': 'Temperatura promedio diaria (¬∞C)',\n",
    "    'HUM_MEAN': 'Humedad relativa promedio diaria (%)',\n",
    "    'PNM_MEAN': 'Presi√≥n atmosf√©rica promedio diaria (hPa)',\n",
    "    'WIND_SPEED_MEAN': 'Velocidad promedio del viento (km/h)'\n",
    "}\n",
    "\n",
    "# Crear subplots para los histogramas\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (var, titulo) in enumerate(variables_hist.items(), 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(df_estaciones_group[var], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(titulo, fontsize=12)\n",
    "    plt.xlabel(titulo)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crear gr√°fico Windrose para WIND_DIR_MEAN\n",
    "df_viento = df_estaciones[(df_estaciones['DD'] <= 360)]\n",
    "\n",
    "ax = WindroseAxes.from_ax()\n",
    "ax.bar(df_viento['DD'], df_viento['FF'], normed=True, opening=0.8, edgecolor='white')\n",
    "\n",
    "# Ajustes de visualizaci√≥n\n",
    "ax.set_title(\"Distribuci√≥n direccional del viento\", fontsize=13)\n",
    "ax.set_legend(loc='lower right', title=\"Frecuencia (%)\", fontsize=9, title_fontsize=10, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285026bf",
   "metadata": {},
   "source": [
    "## An√°lisis exploratorio ‚Äì valores m√°ximos, m√≠nimos, promedio diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores inv√°lidos en DD (mayores a 360)\n",
    "valores_dd_invalidos = df_estaciones[df_estaciones['DD'] > 360]['DD'].unique()\n",
    "print(\"Valores inv√°lidos en DD (mayores a 360):\", valores_dd_invalidos, \"\\n\")\n",
    "\n",
    "# Crear columna de fecha sin hora para agrupar\n",
    "df_estaciones['FECHA_DIA'] = df_estaciones['FECHA'].dt.date\n",
    "\n",
    "# Crear DD_VALID con NaN en valores > 360 (para excluir del promedio)\n",
    "df_estaciones['DD_VALID'] = df_estaciones['DD'].where(df_estaciones['DD'] <= 360, pd.NA)\n",
    "\n",
    "# Paso 3: Agrupar por estaci√≥n y d√≠a, y calcular estad√≠sticas\n",
    "df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA_DIA']).agg({\n",
    "    'TEMP': ['mean', 'min', 'max'],\n",
    "    'PNM': ['mean', 'min', 'max'],\n",
    "    'HUM': ['mean', 'min', 'max'],\n",
    "    'DD_VALID': ['mean'],       # Promedio con valores v√°lidos solamente\n",
    "    'DD': ['min', 'max'],       # Para conservar min y max sin filtrar\n",
    "    'FF': ['mean', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas para facilitar lectura\n",
    "df_estaciones_group.columns = [\n",
    "    'ESTACION', 'FECHA',\n",
    "    'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',\n",
    "    'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',\n",
    "    'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',\n",
    "    'WIND_DIR_MEAN',  # <- Esta es la media con valores v√°lidos\n",
    "    'WIND_DIR_MIN', 'WIND_DIR_MAX',\n",
    "    'WIND_SPEED_MEAN', 'WIND_SPEED_MIN', 'WIND_SPEED_MAX'\n",
    "]\n",
    "\n",
    "# Redondear solo las columnas *_MEAN a 1 decimal\n",
    "cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "df_estaciones_group[cols_mean] = df_estaciones_group[cols_mean].round(1)\n",
    "\n",
    "# Vista previa\n",
    "print(df_estaciones_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571e2cc-4e29-4aab-9b8d-568eb54fb26b",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n por estaci√≥n y por variable\n",
    "\n",
    "Se generan gr√°ficos individuales para cada estaci√≥n meteorol√≥gica, mostrando la evoluci√≥n diaria de temperatura, presi√≥n, humedad y viento. \n",
    "Tambi√©n se incluye una visualizaci√≥n tipo Windrose para observar la distribuci√≥n de la direcci√≥n y velocidad del viento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55458f9-ecc2-4952-a6c7-e75927e6a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que FECHA est√© en datetime\n",
    "df_estaciones_group['FECHA'] = pd.to_datetime(df_estaciones_group['FECHA'])\n",
    "\n",
    "variables = {\n",
    "    'TEMP': {'min': 'TEMP_MIN', 'mean': 'TEMP_MEAN', 'max': 'TEMP_MAX', 'label': 'Temperatura (¬∞C)'},\n",
    "    'HUM': {'min': 'HUM_MIN', 'mean': 'HUM_MEAN', 'max': 'HUM_MAX', 'label': 'Humedad (%)'},\n",
    "    'PNM': {'min': 'PNM_MIN', 'mean': 'PNM_MEAN', 'max': 'PNM_MAX', 'label': 'Presi√≥n (hPa)'},\n",
    "    'WIND_SPEED': {'min': 'WIND_SPEED_MIN', 'mean': 'WIND_SPEED_MEAN', 'max': 'WIND_SPEED_MAX', 'label': 'Velocidad del viento (km/h)'}\n",
    "}\n",
    "\n",
    "# Recorrer estaciones\n",
    "for estacion in df_estaciones_group['ESTACION'].unique():\n",
    "    df_est_group = df_estaciones_group[df_estaciones_group['ESTACION'] == estacion]\n",
    "    df_est_raw = df_estaciones[(df_estaciones['NOMBRE'] == estacion) & (df_estaciones['DD'] <= 360)]\n",
    "\n",
    "    print(f\"\\n Estaci√≥n: {estacion}\")\n",
    "\n",
    "    # Una fila por variable (cada una con 3 columnas: min, mean, max)\n",
    "    for var, datos in variables.items():\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=(\"M√≠nimo\", \"M√°ximo\", \"Promedio\")\n",
    "        )\n",
    "\n",
    "        colores = {\n",
    "            'min': 'blue',\n",
    "            'max': 'red',\n",
    "            'mean': 'green'\n",
    "        }\n",
    "        \n",
    "        for i, tipo in enumerate(['min', 'max', 'mean']):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_est_group['FECHA'],\n",
    "                    y=df_est_group[datos[tipo]],\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{datos['label']} ‚Äì {tipo}\",\n",
    "                    line=dict(color=colores[tipo]),\n",
    "                    hovertemplate=\"Fecha: %{x|%d-%m-%Y}<br>Valor: %{y:.2f}<extra></extra>\"\n",
    "                ),\n",
    "                row=1, col=i + 1\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{datos['label']} ‚Äì {estacion}\",\n",
    "            height=400,\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.show()\n",
    "\n",
    "    # Graficar rosa del viento con matplotlib (se mantiene igual)\n",
    "    ax = WindroseAxes.from_ax()\n",
    "    ax.bar(df_est_raw['DD'], df_est_raw['FF'], normed=True, opening=0.8, edgecolor='white')\n",
    "    ax.set_title(f'Direcci√≥n y velocidad del viento ‚Äì {estacion}', fontsize=12)\n",
    "    ax.set_legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c552a-5154-4e32-8fa9-7507eb684df7",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n Min-Max: ¬øPor qu√© la aplicamos?\n",
    "\n",
    "Cuando trabajamos con m√∫ltiples variables meteorol√≥gicas ‚Äîcomo temperatura, presi√≥n, humedad, direcci√≥n y velocidad del viento‚Äî nos enfrentamos a un problema: **cada una est√° en una escala diferente**.\n",
    "\n",
    "- Temperatura: 5‚ÄØ¬∞C a 35‚ÄØ¬∞C  \n",
    "- Presi√≥n atmosf√©rica: 1000 a 1030 hPa  \n",
    "- Humedad relativa: 40‚ÄØ% a 100‚ÄØ%  \n",
    "- Direcci√≥n del viento: 0¬∞ a 360¬∞  \n",
    "- Velocidad del viento: 0 a 30 km/h  \n",
    "\n",
    "Estas diferencias pueden dificultar tanto la comparaci√≥n visual como el an√°lisis conjunto. Para resolverlo, aplicamos la **normalizaci√≥n Min-Max**, que transforma cada valor al rango [0,‚ÄØ1] manteniendo su proporcionalidad, mediante la siguiente f√≥rmula:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "Donde:  \n",
    "- $x$ es el valor original  \n",
    "- $x_{\\text{min}}$ es el m√≠nimo observado de la variable  \n",
    "- $x_{\\text{max}}$ es el m√°ximo observado de la variable  \n",
    "\n",
    "### ¬øQu√© logramos con esto?\n",
    "\n",
    "- Facilitar la **comparaci√≥n visual** entre variables heterog√©neas  \n",
    "- Preparar los datos para **an√°lisis multivariado** (como clustering o reducci√≥n de dimensiones)  \n",
    "- Evitar que variables con mayor escala **dominen** sobre otras en modelos o gr√°ficos  \n",
    "- Posibilitar visualizaciones integradas, como **gr√°ficos radar o l√≠neas normalizadas**\n",
    "\n",
    "> Esta transformaci√≥n no modifica la forma de la distribuci√≥n, sino √∫nicamente su escala, lo que permite trabajar todas las variables en igualdad de condiciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cantidad de valores nulos antes del rellenado\n",
    "print(\"üîç Valores nulos por columna antes del forward fill:\\n\")\n",
    "print(df_estaciones_group.isnull().sum())\n",
    "\n",
    "# Rellenar valores nulos con forward fill\n",
    "df_estaciones_group.ffill(inplace=True)\n",
    "\n",
    "# Definir columnas *_MEAN para normalizar\n",
    "cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "\n",
    "# Aplicar normalizaci√≥n Min-Max y mostrar ejemplos\n",
    "for col in cols_mean:\n",
    "    min_val = df_estaciones_group[col].min()\n",
    "    max_val = df_estaciones_group[col].max()\n",
    "    df_estaciones_group[col + '_NORM'] = (df_estaciones_group[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Imprimir resumen\n",
    "    print(f\"\\nüîπ Normalizaci√≥n de {col}:\")\n",
    "    print(f\"   Valor m√≠nimo: {min_val:.2f}, m√°ximo: {max_val:.2f}\")\n",
    "    print(df_estaciones_group[[col, col + '_NORM']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19408960",
   "metadata": {},
   "source": [
    "## Exportaci√≥n a Capa Plata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar la Capa Plata:\n",
    "df_estaciones_group.to_csv(PLATA_DIR / 'dataset_plata_inicial.csv', index=False)\n",
    "\n",
    "# Imprimir resumen de exportaci√≥n\n",
    "print(\"\\n Archivos exportados en formato Capa Plata:\")\n",
    "print(f\" - CSV:     {PLATA_DIR / 'dataset_plata_inicial.csv'}\")\n",
    "print(f\"\\n Filas exportadas: {len(df_estaciones_group)}\")\n",
    "print(f\" Columnas exportadas: {len(df_estaciones_group.columns)}\")\n",
    "print(f\"\\n Vista previa:\")\n",
    "print(df_estaciones_group.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ed9e3-20f8-4c55-a01c-9d7651fb9af3",
   "metadata": {},
   "source": [
    "## Generaci√≥n de metadatos y documentaci√≥n del dataset\n",
    "\n",
    "Para garantizar la trazabilidad y comprensi√≥n del dataset procesado, se generan tres archivos complementarios:\n",
    "\n",
    "- **Metadatos de variables**: resumen estad√≠stico por columna (tipo, nulos, valores √∫nicos, m√≠nimos, m√°ximos, etc.).\n",
    "- **Diccionario de variables**: descripci√≥n manual del significado, unidad y observaciones de cada campo.\n",
    "- **Metadatos generales**: informaci√≥n sobre la fuente, cobertura geogr√°fica y temporal, formato, y responsable del procesamiento.\n",
    "\n",
    "Estos archivos permiten documentar correctamente la Capa Plata, facilitando su interpretaci√≥n y reutilizaci√≥n en etapas posteriores del pipeline de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b44574-5fea-4a88-860f-257b6360f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funci√≥n para generar metadatos por columna ---\n",
    "def generar_metadatos(df):\n",
    "    metadatos = []\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        tipo = serie.dtype\n",
    "        no_nulos = serie.notnull().sum()\n",
    "        nulos = serie.isnull().sum()\n",
    "        pct_nulos = (nulos / len(serie)) * 100\n",
    "        unicos = serie.nunique()\n",
    "        ejemplo = serie.dropna().iloc[0] if no_nulos > 0 else None\n",
    "        try:\n",
    "            minimo = serie.min()\n",
    "            maximo = serie.max()\n",
    "        except:\n",
    "            minimo, maximo = None, None\n",
    "\n",
    "        metadatos.append({\n",
    "            'Columna': col,\n",
    "            'Tipo de dato': str(tipo),\n",
    "            'Valores no nulos': no_nulos,\n",
    "            'Valores nulos': nulos,\n",
    "            '% Nulos': round(pct_nulos, 2),\n",
    "            'Valor m√≠nimo': minimo,\n",
    "            'Valor m√°ximo': maximo,\n",
    "            'Valores √∫nicos': unicos,\n",
    "            'Ejemplo': ejemplo\n",
    "        })\n",
    "    return pd.DataFrame(metadatos)\n",
    "\n",
    "# --- Diccionario de variables (manual) ---\n",
    "diccionario_vars = pd.DataFrame([\n",
    "    [\"ESTACION\", \"Nombre de la estaci√≥n meteorol√≥gica\", \"-\", \"Agrupaci√≥n principal\"],\n",
    "    [\"FECHA\", \"Fecha de la medici√≥n\", \"AAAA-MM-DD\", \"Convertido desde DDMMAAAA\"],\n",
    "    [\"TEMP_MIN\", \"Temperatura m√≠nima diaria\", \"¬∞C\", \"\"],\n",
    "    [\"TEMP_MEAN\", \"Temperatura promedio diaria\", \"¬∞C\", \"\"],\n",
    "    [\"TEMP_MAX\", \"Temperatura m√°xima diaria\", \"¬∞C\", \"\"],\n",
    "    [\"PNM_MIN\", \"Presi√≥n m√≠nima diaria\", \"hPa\", \"\"],\n",
    "    [\"PNM_MEAN\", \"Presi√≥n promedio diaria\", \"hPa\", \"\"],\n",
    "    [\"PNM_MAX\", \"Presi√≥n m√°xima diaria\", \"hPa\", \"\"],\n",
    "    [\"HUM_MIN\", \"Humedad relativa m√≠nima diaria\", \"%\", \"\"],\n",
    "    [\"HUM_MEAN\", \"Humedad relativa promedio diaria\", \"%\", \"\"],\n",
    "    [\"HUM_MAX\", \"Humedad relativa m√°xima diaria\", \"%\", \"\"],\n",
    "    [\"WIND_DIR_MEAN\", \"Direcci√≥n promedio del viento\", \"¬∞\", \"Se excluyeron valores > 360\"],\n",
    "    [\"WIND_SPEED_MEAN\", \"Velocidad promedio del viento\", \"km/h\", \"\"],\n",
    "    [\"TEMP_MEAN_NORM\", \"TEMP_MEAN normalizada Min-Max\", \"[0, 1]\", \"Para an√°lisis multivariado\"],\n",
    "    [\"PNM_MEAN_NORM\", \"PNM_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"HUM_MEAN_NORM\", \"HUM_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"WIND_DIR_MEAN_NORM\", \"WIND_DIR_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"WIND_SPEED_MEAN_NORM\", \"WIND_SPEED_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"]\n",
    "], columns=[\"Columna\", \"Descripci√≥n\", \"Unidad\", \"Observaciones\"])\n",
    "\n",
    "# --- Metadatos generales autom√°ticos ---\n",
    "estaciones = df_estaciones['NOMBRE'].unique()\n",
    "provincia = estaciones[0].split()[-1].capitalize()\n",
    "cobertura_geo = f\"Estaciones meteorol√≥gicas de la provincia de {provincia}, Argentina\"\n",
    "\n",
    "fecha_min = df_estaciones['FECHA'].min().strftime('%Y-%m-%d')\n",
    "fecha_max = df_estaciones['FECHA'].max().strftime('%Y-%m-%d')\n",
    "cobertura_temporal = f\"Desde {fecha_min} hasta {fecha_max}\"\n",
    "\n",
    "metadatos_generales = pd.DataFrame([\n",
    "    [\"Nombre del conjunto de datos\", \"misiones_plata\"],\n",
    "    [\"Fuente original\", \"Servicio Meteorol√≥gico Nacional (SMN)\"],\n",
    "    [\"Cobertura geogr√°fica\", cobertura_geo],\n",
    "    [\"Cobertura temporal\", cobertura_temporal],\n",
    "    [\"Frecuencia\", \"Datos horarios, agregados a diario\"],\n",
    "    [\"Unidad de observaci√≥n\", \"Estaci√≥n meteorol√≥gica\"],\n",
    "    [\"Formato de archivo\", \"CSV, Parquet, TXT (delimitado por tabulaciones)\"],\n",
    "    [\"Fecha de procesamiento\", date.today().isoformat()],\n",
    "    [\"Responsable del procesamiento\", \"Equipo de an√°lisis de datos - FIUBA\"],\n",
    "    [\"Nivel del dataset\", \"Capa Plata (datos limpios, normalizados y listos para an√°lisis)\"]\n",
    "], columns=[\"Campo\", \"Valor\"])\n",
    "\n",
    "# --- Exportar los tres archivos ---\n",
    "metadatos_df = generar_metadatos(df_estaciones_group)\n",
    "path_metadatos = DICCIONARIO_DIR / 'metadatos_variables.csv'\n",
    "path_diccionario = DICCIONARIO_DIR / 'diccionario_variables.csv'\n",
    "path_generales = DICCIONARIO_DIR / 'metadatos_generales.csv'\n",
    "\n",
    "metadatos_df.to_csv(path_metadatos, index=False)\n",
    "diccionario_vars.to_csv(path_diccionario, index=False)\n",
    "metadatos_generales.to_csv(path_generales, index=False)\n",
    "\n",
    "# --- Resumen por consola ---\n",
    "print(\"\\n Metadatos exportados:\")\n",
    "print(f\" - {path_metadatos.name} ({len(metadatos_df)} columnas analizadas)\")\n",
    "print(f\" - {path_generales.name} (resumen general del dataset)\")\n",
    "\n",
    "print(\"\\n Diccionario de variables exportado:\")\n",
    "print(f\" - {path_diccionario.name} ({len(diccionario_vars)} filas)\")\n",
    "\n",
    "print(\"\\n Vista previa de los archivos generados:\\n\")\n",
    "\n",
    "print(\"üîπ Metadatos por variable:\")\n",
    "print(pd.read_csv(path_metadatos).head(3), \"\\n\")\n",
    "\n",
    "print(\"üîπ Diccionario de variables:\")\n",
    "print(pd.read_csv(path_diccionario).head(3), \"\\n\")\n",
    "\n",
    "print(\"üîπ Metadatos generales:\")\n",
    "print(pd.read_csv(path_generales).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d08d86-b3a1-44bb-bbfd-d4dad7c8f67c",
   "metadata": {},
   "source": [
    "# Conclusi√≥n\n",
    "\n",
    "Durante esta clase construimos la **Capa Plata** del pipeline de datos aplicando buenas pr√°cticas de limpieza y exploraci√≥n. Logramos:\n",
    "\n",
    "- Transformar fechas y crear una columna temporal (`FECHA_HORA`).\n",
    "- Calcular m√©tricas diarias por estaci√≥n para temperatura, presi√≥n, humedad y viento.\n",
    "- Filtrar valores inv√°lidos (como direcciones de viento > 360).\n",
    "- Normalizar variables para facilitar comparaciones entre estaciones.\n",
    "- Visualizar el comportamiento de cada variable y la direcci√≥n predominante del viento.\n",
    "- Exportar el conjunto resultante en m√∫ltiples formatos (CSV, Parquet, TXT).\n",
    "\n",
    "Este dataset enriquecido ser√° la base para los an√°lisis posteriores en las pr√≥ximas clases (Clases 5 a 7), incluyendo miner√≠a de datos, clasificaci√≥n y visualizaci√≥n avanzada.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

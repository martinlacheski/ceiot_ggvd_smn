{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303ce6d",
   "metadata": {},
   "source": [
    "# Ingesta y Capa Bronce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a3d2d",
   "metadata": {},
   "source": [
    "En esta notebook se inicia la construcción del pipeline de datos meteorológicos, trabajando con los archivos crudos provistos por el SMN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87778",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573fb540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importación de librerías completada.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Importación de librerías completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47201f0b",
   "metadata": {},
   "source": [
    "## Configuración de paths y carpetas del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069e436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciación de carpetas del proyecto completada.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "BRONCE_DIR = BASE_DIR / 'data' / 'bronce'\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "for path in [BRONCE_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Iniciación de carpetas del proyecto completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba45b",
   "metadata": {},
   "source": [
    "## Lectura del archivo de estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fd630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estaciones cargadas: 117\n",
      "Cantidad de provincias: 26\n",
      "Provincias disponibles: ['ANTARTIDA' 'BUENOS AIRES' 'CAPITAL FEDERAL' 'CATAMARCA' '' 'CHACO'\n",
      " 'CHUBUT' 'CORDOBA' 'CORRIENTES' 'ENTRE RIOS' 'FORMOSA' 'JUJUY' 'LA PAMPA'\n",
      " 'LA RIOJA' 'MENDOZA' 'MISIONES' 'NEUQUEN' 'RIO NEGRO' 'SALTA' 'SAN JUAN'\n",
      " 'SAN LUIS' 'SANTA CRUZ' 'SANTA FE' 'SANTIAGO DEL ESTERO'\n",
      " 'TIERRA DEL FUEGO' 'TUCUMAN']\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo\n",
    "archivo_estaciones = RAW_DIR / 'estaciones' / 'estaciones_smn.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_estaciones, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()[2:]\n",
    "\n",
    "# Expresión regular para extraer campos:\n",
    "pattern = re.compile(\n",
    "    r\"^(?P<nombre>.+?)\\s{2,}(?P<provincia>.+?)\\s{2,}(?P<lat_gr>-?\\d+)\\s+(?P<lat_min>\\d+)\\s+(?P<lon_gr>-?\\d+)\\s+(?P<lon_min>\\d+)\\s+(?P<altura_m>\\d+)\\s+(?P<numero>\\d+)\\s+(?P<numero_oaci>\\S+)\\s*$\"\n",
    ")\n",
    "\n",
    "# Extraer los datos\n",
    "data = []\n",
    "for line in lines:\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        data.append(match.groupdict())\n",
    "\n",
    "# Crear DataFrame\n",
    "df_estaciones = pd.DataFrame(data)\n",
    "\n",
    "# Conversión de tipos\n",
    "df_estaciones[['lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero']] = df_estaciones[[\n",
    "    'lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero'\n",
    "]].apply(pd.to_numeric)\n",
    "\n",
    "# Cargar las provincias\n",
    "provincias_unicas = df_estaciones['provincia'].str.strip().str.upper().unique()\n",
    "\n",
    "# Imprimir la cantidad de estaciones registradas\n",
    "print(\"Estaciones cargadas:\", len(df_estaciones))\n",
    "\n",
    "# Imprimir la cantidad de provincias registradas\n",
    "print(\"Cantidad de provincias:\", len(provincias_unicas))\n",
    "\n",
    "# Imprimir las provincias\n",
    "print(\"Provincias disponibles:\", provincias_unicas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9de26",
   "metadata": {},
   "source": [
    "## Selección de estaciones. \n",
    "\n",
    "### Para el desarrollo del trabajo se utilizarán las estaciones ubicadas en la provincia de Neuquén"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355e73a4-ae19-4a13-9709-93f131a03792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>provincia</th>\n",
       "      <th>numero</th>\n",
       "      <th>numero_oaci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>CHAPELCO AERO</td>\n",
       "      <td>NEUQUEN</td>\n",
       "      <td>87761</td>\n",
       "      <td>SAZY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NEUQUEN AERO</td>\n",
       "      <td>NEUQUEN</td>\n",
       "      <td>87715</td>\n",
       "      <td>SAZN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nombre provincia  numero numero_oaci\n",
       "81  CHAPELCO AERO   NEUQUEN   87761        SAZY\n",
       "82   NEUQUEN AERO   NEUQUEN   87715        SAZN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ingresar el nombre de la provincia con la que se va a trabajar\n",
    "provincia = 'NEUQUEN'\n",
    "\n",
    "df_provincia = df_estaciones[df_estaciones['provincia'].str.upper() == provincia]\n",
    "df_provincia[['nombre', 'provincia', 'numero', 'numero_oaci']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a92af-1e2b-4851-8929-6e6a8476500c",
   "metadata": {},
   "source": [
    "## Filtrar las estaciones que correspondan a la provincia seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac45e6ae-4c6a-422b-80e2-3125b1a5f206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FECHA HORA TEMP HUM    PNM  DD FF        NOMBRE\n",
      "31052025    0  3.0  69 1025.9 270  7 CHAPELCO AERO\n",
      "31052025    1  3.0  69 1025.5   0  0 CHAPELCO AERO\n",
      "31052025    2  3.0  69 1025.2 290 20 CHAPELCO AERO\n",
      "31052025    3  2.0  80 1025.2 270 13 CHAPELCO AERO\n",
      "31052025    4  2.4  74 1024.9 270 17 CHAPELCO AERO\n",
      "31052025    5  2.4  74 1025.0 250 13 CHAPELCO AERO\n",
      "31052025    6  2.0  80 1024.5 250 20 CHAPELCO AERO\n",
      "31052025    7  2.0  77 1024.8 250 19 CHAPELCO AERO\n",
      "31052025    8  1.8  76 1024.9 260 24 CHAPELCO AERO\n",
      "31052025    9  2.0  77 1024.9 270 17 CHAPELCO AERO\n",
      "31052025   10  2.6  71 1024.9 250 17 CHAPELCO AERO\n",
      "31052025   11  3.8  66 1024.6 290 28 CHAPELCO AERO\n",
      "31052025   12  5.4  58 1024.4 260 11 CHAPELCO AERO\n",
      "31052025   13  7.0  48 1023.4 250 13 CHAPELCO AERO\n",
      "31052025   14  9.2  36 1022.1 270 20 CHAPELCO AERO\n",
      "31052025   15 10.6  29 1021.4 290 28 CHAPELCO AERO\n",
      "31052025   16 10.4  33 1020.9 270 13 CHAPELCO AERO\n",
      "31052025   17 10.4  31 1020.9 270 13 CHAPELCO AERO\n",
      "31052025   18  9.4  35 1021.3 240  9 CHAPELCO AERO\n",
      "31052025   19  7.8  35 1022.0 230 11 CHAPELCO AERO\n",
      "31052025   20  5.2  55 1022.8 260  6 CHAPELCO AERO\n",
      "31052025   21  4.3  66 1023.1   0  0 CHAPELCO AERO\n",
      "31052025   22  4.2  58 1023.3 270  9 CHAPELCO AERO\n",
      "31052025   23  2.8  65 1023.0 990  6 CHAPELCO AERO\n",
      "31052025    0  8.6  51 1024.2 270  9  NEUQUEN AERO\n",
      "31052025    1  8.2  50 1023.9 230 19  NEUQUEN AERO\n",
      "31052025    2  7.6  48 1023.8 270  7  NEUQUEN AERO\n",
      "31052025    3  6.7  53 1023.4 270 11  NEUQUEN AERO\n",
      "31052025    4  5.0  65 1023.6   0  0  NEUQUEN AERO\n",
      "31052025    5  3.4  76 1024.1 270  7  NEUQUEN AERO\n",
      "31052025    6  2.2  72 1024.2   0  0  NEUQUEN AERO\n",
      "31052025    7  2.2  72 1024.2 290  7  NEUQUEN AERO\n",
      "31052025    8  3.2  73 1024.5 320  6  NEUQUEN AERO\n",
      "31052025    9  1.6  78 1025.0 290  9  NEUQUEN AERO\n",
      "31052025   10  4.1  63 1025.0   0  0  NEUQUEN AERO\n",
      "31052025   11  7.0  54 1024.9 290 11  NEUQUEN AERO\n",
      "31052025   12  9.2  53 1024.7 250 11  NEUQUEN AERO\n",
      "31052025   13 11.0  45 1023.9 230 15  NEUQUEN AERO\n",
      "31052025   14 12.6  38 1022.9 250 11  NEUQUEN AERO\n",
      "31052025   15 13.6  31 1021.9 250 13  NEUQUEN AERO\n",
      "31052025   16 14.4  31 1021.6 250 11  NEUQUEN AERO\n",
      "31052025   17 14.0  31 1021.8 270 11  NEUQUEN AERO\n",
      "31052025   18 13.1  33 1021.8 270  6  NEUQUEN AERO\n",
      "31052025   19  8.7  52 1022.3   0  0  NEUQUEN AERO\n",
      "31052025   20  5.6  66 1022.5   0  0  NEUQUEN AERO\n",
      "31052025   21  3.7  48 1023.0   0  0  NEUQUEN AERO\n",
      "31052025   22  1.7  69 1023.2 290  4  NEUQUEN AERO\n",
      "31052025   23  1.0  70 1023.2   0  0  NEUQUEN AERO\n",
      "\n",
      "Columnas: ['FECHA', 'HORA', 'TEMP', 'HUM', 'PNM', 'DD', 'FF', 'NOMBRE']\n",
      "Tipos de dato:\n",
      "FECHA      object\n",
      "HORA        Int64\n",
      "TEMP      float64\n",
      "HUM         int64\n",
      "PNM       float64\n",
      "DD          Int64\n",
      "FF          Int64\n",
      "NOMBRE     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Se selecciona una fecha para visualizar los datos\n",
    "archivo_dato = RAW_DIR / 'datohorario' /  '_procesados' / 'datohorario20250531.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_dato, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Detectar columnas separadas por múltiples espacios\n",
    "columnas = re.split(r\"\\s{2,}\", lines[0].strip())\n",
    "\n",
    "# Leer datos\n",
    "data = [\n",
    "    re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "    for line in lines[1:]\n",
    "    if len(line.strip()) > 0 and not line.isspace()\n",
    "]\n",
    "\n",
    "# Crear DataFrame con columnas originales\n",
    "df_dato = pd.DataFrame(data, columns=columnas)\n",
    "df_dato.columns = df_dato.columns.str.strip()\n",
    "df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "# Filtrar por estaciones\n",
    "nombres_provincia = df_provincia[\"nombre\"].str.strip().unique()\n",
    "df_provincia_dia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "# Crear copia y convertir tipos SOLO para impresión de tipos correctos\n",
    "df_tipos = df_provincia_dia.copy()\n",
    "df_tipos[\"FECHA\"] = pd.to_datetime(df_tipos[\"FECHA\"], format=\"%d%m%Y\", errors=\"coerce\").dt.date\n",
    "df_tipos[\"HORA\"] = pd.to_numeric(df_tipos[\"HORA\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"TEMP\"] = pd.to_numeric(df_tipos[\"TEMP\"], errors=\"coerce\")\n",
    "df_tipos[\"HUM\"] = pd.to_numeric(df_tipos[\"HUM\"], errors=\"coerce\")\n",
    "df_tipos[\"PNM\"] = pd.to_numeric(df_tipos[\"PNM\"], errors=\"coerce\")\n",
    "df_tipos[\"DD\"] = pd.to_numeric(df_tipos[\"DD\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"FF\"] = pd.to_numeric(df_tipos[\"FF\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Mostrar todos los resultados\n",
    "print(df_provincia_dia.to_string(index=False))\n",
    "print()\n",
    "print(\"Columnas:\", df_dato.columns.tolist())\n",
    "print(\"Tipos de dato:\")\n",
    "print(df_tipos.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832851a",
   "metadata": {},
   "source": [
    "## Procesamiento por estación y por fecha (con limpieza y reporte resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5af8f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado.\n",
      "Días procesados: 422\n",
      "Errores al procesar archivos: 0\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta de salida si no existe\n",
    "BRONCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos datohorario disponibles\n",
    "archivos_datos = sorted(glob(str(RAW_DIR / \"datohorario\" / '_procesados' / \"datohorario*.txt\")))\n",
    "\n",
    "errores_globales = 0\n",
    "\n",
    "for archivo in archivos_datos:\n",
    "    try:\n",
    "        with open(archivo, encoding=\"latin1\") as f:\n",
    "            raw_lines = f.readlines()\n",
    "\n",
    "        header = raw_lines[0].strip()\n",
    "        columnas = re.split(r\"\\s{2,}\", header)\n",
    "\n",
    "        data = [\n",
    "            re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "            for line in raw_lines[1:]\n",
    "            if len(line.strip()) > 0 and not line.isspace()\n",
    "        ]\n",
    "\n",
    "        df_dato = pd.DataFrame(data, columns=columnas)\n",
    "        df_dato.columns = df_dato.columns.str.strip()\n",
    "        df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "        # Filtrar por estaciones según la provincia\n",
    "        df_provincia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "        # Obtener fecha\n",
    "        fecha = Path(archivo).stem.replace(\"datohorario\", \"\")\n",
    "\n",
    "        # Guardar archivos por estación\n",
    "        for nombre in nombres_provincia:\n",
    "            nombre_clean = nombre.lower().replace(\" \", \"_\")\n",
    "            df_estacion = df_provincia[df_provincia[\"NOMBRE\"] == nombre]\n",
    "\n",
    "            if not df_estacion.empty:\n",
    "                path_estacion = BRONCE_DIR / nombre_clean\n",
    "                path_estacion.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Archivos de salida\n",
    "                archivo_csv = path_estacion / f\"{fecha}.csv\"\n",
    "                df_estacion.to_csv(archivo_csv, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        errores_globales += 1\n",
    "        continue\n",
    "\n",
    "# Reporte final\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Días procesados: {len(archivos_datos)}\")\n",
    "print(f\"Errores al procesar archivos: {errores_globales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9c0bc-09c9-4e51-9ef8-e975403b2422",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En este notebook realizamos el proceso de **ingesta de datos meteorológicos** y la creación de la **Capa Bronce** de nuestro proyecto:\n",
    "\n",
    "1. **Lectura de datos crudos** provenientes de archivos del Servicio Meteorológico Nacional.\n",
    "2. **Estructuración inicial de datos**, manteniendo la información tal como llega del entorno real, sin limpieza ni transformación.\n",
    "3. **Organización en la estructura de carpetas** del proyecto, asegurando que los datos queden almacenados en la capa correspondiente para futuras etapas de procesamiento.\n",
    "\n",
    "Esta etapa constituye la **base del pipeline de datos**, preservando la trazabilidad y sirviendo como fuente de verdad para las capas posteriores (Plata y Oro)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
